{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Lambda, Cropping2D, AveragePooling2D, Convolution2D, Dropout, ELU\n",
    "\n",
    "print('imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images normalized to [-1,+1]\n",
      "(160, 320, 3) image dimensions, cropped by  65 px from top and  15 px from bot\n",
      "47586 training images out of  55984 overall images [ 15.0 % for validation set]\n",
      "250 batch-size,  0.025 learning rate,  1e-05 decay,  0.85 nesterov-momentum\n",
      "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 20 %, Layer 1-2: 45 %\n",
      "Epoch 1/15\n",
      "42/95 [============>.................] - ETA: 290s - loss: 0.0738"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0f5b3b45ed75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[0msgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnesterov_momentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msgd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1105\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1107\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1844\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1845\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1846\u001b[1;33m                             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Settings\n",
    "split=0.15        #Split overall image data into training and validation set\n",
    "resize_factor=1   #resize the images by this factor *actually has no effect anymore, as 2x2-average-pooling is used as resizing by half\n",
    "crop_top=65       #crop uninteresting area, like far away objects and skyline\n",
    "crop_bot=15       #crop uninteresting area that is too near or the hood\n",
    "dropout1=0.02     #dropout-rate between layer 1 and 2\n",
    "dropout2=0.03     #dropout-rate between layer 2 and 3\n",
    "dropout3=0.20     #dropout-rate between layer 3 and 4\n",
    "dropout4=0.45     #dropout-rate between layer 4 and 5\n",
    "learning_rate=0.025\n",
    "decay=1e-5\n",
    "nesterov_momentum=0.85\n",
    "batch=250         #Batch size of images that are computed at once\n",
    "epochs=15         #Number of epochs the network is trained \n",
    "\n",
    "#resize function, because lambda need tf be imported at each call, else it causes an error\n",
    "#def resize_image(images,factor,in_height,in_width):\n",
    "#    from keras.backend import tf as ktf\n",
    "#return ktf.image.resize_images(images, (int(in_height*factor), int(in_width*factor)))\n",
    "\n",
    "\n",
    "samples = []\n",
    "with open('M:/records p3/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "#Split data into training and validation sets\n",
    "train_samples, validation_samples = train_test_split(samples, test_size=split)\n",
    "\n",
    "def generator(samples, batch_size=batch): #As the image data is mirrored, twice the batch_size is loaded at the same time\n",
    "    num_samples = len(samples)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            measurements = []\n",
    "            for batch_sample in batch_samples:\n",
    "                name = batch_sample[0]                 # 'M:/records p3/IMG/...' [0]for center camera\n",
    "                image_data = cv2.imread(name)\n",
    "                image_data = cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)# switch it to RGB, as drive.py sends RGB\n",
    "                steering_angle = float(batch_sample[3])\n",
    "                images.append(image_data)\n",
    "                measurements.append(steering_angle)\n",
    "                ##### mirror data vertically #######################\n",
    "                image_data=np.fliplr(image_data)\n",
    "                images.append(image_data)\n",
    "                steering_angle=-steering_angle\n",
    "                measurements.append(steering_angle)\n",
    "\n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(measurements)\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=batch)\n",
    "validation_generator = generator(validation_samples, batch_size=batch)\n",
    "\n",
    "in_shape=((cv2.imread(samples[20][0])).shape[0],(cv2.imread(samples[20][0])).shape[1],(cv2.imread(samples[20][0])).shape[2])\n",
    "\n",
    "####print informations for documentation####\n",
    "print('Images normalized to [-1,+1]')\n",
    " #print(in_shape, 'original image dimensions, resized to', (int(in_shape[0]*resize_factor), int(in_shape[1]*resize_factor)))\n",
    "print(in_shape, 'image dimensions, cropped by ', crop_top, 'px from top and ', crop_bot, 'px from bot')\n",
    "print(int(len(samples)*2*(1-split)),'training images out of ',len(samples)*2, 'overall images [',split*100,'% for validation set]' )\n",
    "print(batch, 'batch-size, ',learning_rate,'learning rate, ', decay,'decay, ', nesterov_momentum,'nesterov-momentum')\n",
    "print('Dropout-Rates: Layer 1-2:',int(dropout1*100),'%, Layer 1-2:',int(dropout2*100),'%, Layer 1-2:',int(dropout3*100),'%, Layer 1-2:',int(dropout4*100),'%')\n",
    "##############################\n",
    "#Neural Network Architecture\n",
    "##############################\n",
    "model=Sequential()\n",
    "model.add(Lambda(lambda X_train: ((X_train/127.5)-1), input_shape=in_shape)) #Normalization to [-1, ... ,+1]\n",
    "model.add(Cropping2D(cropping=((crop_top,crop_bot), (0,0)), input_shape=in_shape)) #Cropping uninteresting image area\n",
    "#model.add(AveragePooling2D(pool_size=(2, 2), strides=2, data_format='channels_first')) #Resizing the image by half according to: https://discussions.udacity.com/t/issues-resizing-images/228196/22\n",
    "#model.add(Lambda(lambda X_train: resize_image(X_train, resize_factor, in_shape[0], in_shape[1]))) \n",
    "\n",
    "##CNN Architecture taken from (c) by Nvidia, implemented by (c) Udacity \n",
    "#model.add(Convolution2D(24,(5,5), strides=(2,2),activation=\"relu\"))\n",
    "#model.add(Dropout(dropout1))\n",
    "#model.add(Convolution2D(36,(5,5), strides=(2,2),activation=\"relu\"))\n",
    "#model.add(Dropout(dropout2))\n",
    "#model.add(Convolution2D(48,(5,5), strides=(2,2),activation=\"relu\"))\n",
    "#model.add(Dropout(dropout3))\n",
    "#model.add(Convolution2D(64,(3,3), activation=\"relu\"))\n",
    "#model.add(Dropout(dropout4))\n",
    "#model.add(Convolution2D(64,(3,3), activation=\"relu\"))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(100))\n",
    "#model.add(Dense(50))\n",
    "#model.add(Dense(1))\n",
    "\n",
    "##VGG16 as mentioned in cheatsheet (https://github.com/commaai/research/blob/master/train_steering_model.py),\n",
    "###with a few changes\n",
    "model.add(Convolution2D(16,(8,8), strides=(4, 4), padding=\"same\",activation=\"relu\"))\n",
    "model.output_shape\n",
    "model.add(Dropout(dropout1))\n",
    "model.add(Convolution2D(32,(5,5), strides=(2, 2), padding=\"same\",activation=\"relu\"))\n",
    "model.output_shape\n",
    "model.add(Dropout(dropout2))\n",
    "model.add(Convolution2D(64,(5,5), strides=(2, 2), padding=\"same\",activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(dropout3))\n",
    "model.add(ELU())\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(dropout4))\n",
    "model.add(ELU())\n",
    "#model.add(Dense(250))\n",
    "model.add(Dense(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "sgd = optimizers.SGD(lr=learning_rate, decay=decay, momentum=nesterov_momentum, nesterov=True)\n",
    "model.compile(loss='mse', optimizer=sgd)\n",
    "model.fit_generator(train_generator, steps_per_epoch=len(train_samples)/batch,epochs=epochs, validation_data=validation_generator, validation_steps=len(validation_samples)/batch)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-156435b5a617>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Reiners/Desktop/p3_driving/windows_sim/model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'modelvisualization.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names)\u001b[0m\n\u001b[0;32m     98\u001b[0m                \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                show_layer_names=True):\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rankdir'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         raise ImportError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     18\u001b[0m                           ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "\n",
    "model=load_model('C:/Users/Reiners/Desktop/p3_driving/windows_sim/model.h5')\n",
    "\n",
    "plot_model(model, to_file='modelvisualization.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First serious approach\n",
    "- 18858x2 images, 15% validation-set-split\n",
    "- normalized to [-1,+1], original sized\n",
    "- Generator with batchsize=800\n",
    "- Most basic NN architecture from lesson\n",
    "---\n",
    "    Epoch 1/5 loss: 40.2280 - val_loss: 11.4453\n",
    "    Epoch 2/5 loss: 4.5891 - val_loss: 1.2830\n",
    "    Epoch 3/5 loss: 0.9326 - val_loss: 0.6150\n",
    "    Epoch 4/5 loss: 0.3673 - val_loss: 0.2114\n",
    "    Epoch 5/5 loss: 0.1359 - val_loss: 0.1609\n",
    "No overfitting and still reducing loss, driving ok, but drives offroad and keeps driving next to the curbs offroad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) original image dimensions, resized to (160, 320)\n",
    "Image cropped by  65 px from top and  15 px from bot\n",
    "Dropout-Rates: Layer 1-2: 1 %, Layer 1-2: 2 %, Layer 1-2: 30 %, Layer 1-2: 50 %\n",
    "33644 training images out of  39582 overall images [ 15.0 % for validation set]\n",
    "learning=0.008, decay=1e-6, nesterovmomentum=0.4\n",
    "250 batch-size\n",
    "Epoch 1/10\n",
    "68/67 [==============================] - 61s - loss: 0.0364 - val_loss: 0.0364\n",
    "[...]\n",
    "Epoch 8/10\n",
    "68/67 [==============================] - 54s - loss: 0.0262 - val_loss: 0.0279\n",
    "Epoch 9/10\n",
    "68/67 [==============================] - 54s - loss: 0.0262 - val_loss: 0.0276\n",
    "Epoch 10/10\n",
    "68/67 [==============================] - 54s - loss: 0.0258 - val_loss: 0.0274\n",
    "\n",
    "leaves curve because of not tunrning enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) original image dimensions, resized to (160, 320)\n",
    "Image cropped by  65 px from top and  15 px from bot\n",
    "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 18 %, Layer 1-2: 36 %\n",
    "33644 training images out of  39582 overall images [ 15.0 % for validation set]\n",
    "learning=0.015, decay=1e-6, nesterovmomentum=0.7\n",
    "250 batch-size\n",
    "Epoch 1/5\n",
    "68/67 [==============================] - 60s - loss: 0.0323 - val_loss: 0.0252\n",
    "Epoch 2/5\n",
    "68/67 [==============================] - 55s - loss: 0.0270 - val_loss: 0.0239\n",
    "Epoch 3/5\n",
    "68/67 [==============================] - 58s - loss: 0.0260 - val_loss: 0.0231\n",
    "Epoch 4/5\n",
    "68/67 [==============================] - 63s - loss: 0.0253 - val_loss: 0.0225\n",
    "Epoch 5/5\n",
    "68/67 [==============================] - 62s - loss: 0.0247 - val_loss: 0.0220\n",
    "\n",
    "till bridge ok, on bridge allways into wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vgg16\n",
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) image dimensions, cropped by  65 px from top and  15 px from bot\n",
    "37777 training images out of  44444 overall images [ 15.0 % for validation set]\n",
    "250 batch-size,  0.015 learning rate,  1e-06 decay,  0.7 nesterov-momentum\n",
    "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 18 %, Layer 1-2: 36 %\n",
    "Epoch 1/5\n",
    "76/75 [==============================] - 58s - loss: 0.0403 - val_loss: 0.0277\n",
    "Epoch 2/5\n",
    "76/75 [==============================] - 54s - loss: 0.0324 - val_loss: 0.0259\n",
    "Epoch 3/5\n",
    "76/75 [==============================] - 54s - loss: 0.0305 - val_loss: 0.0247\n",
    "Epoch 4/5\n",
    "76/75 [==============================] - 54s - loss: 0.0295 - val_loss: 0.0239\n",
    "Epoch 5/5\n",
    "76/75 [==============================] - 54s - loss: 0.0285 - val_loss: 0.0232\n",
    "\n",
    "good, a bit too weak turning in corners, bridge perfect, tight corner approached with very low steering angle and drives off road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) image dimensions, cropped by  65 px from top and  15 px from bot\n",
    "37777 training images out of  44444 overall images [ 15.0 % for validation set]\n",
    "250 batch-size,  0.02 learning rate,  1e-05 decay,  0.85 nesterov-momentum\n",
    "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 18 %, Layer 1-2: 36 %\n",
    "Epoch 1/5\n",
    "76/75 [==============================] - 171s - loss: 0.0358 - val_loss: 0.0273\n",
    "Epoch 2/5\n",
    "76/75 [==============================] - 158s - loss: 0.0287 - val_loss: 0.0249\n",
    "Epoch 3/5\n",
    "76/75 [==============================] - 60s - loss: 0.0270 - val_loss: 0.0238\n",
    "Epoch 4/5\n",
    "76/75 [==============================] - 59s - loss: 0.0259 - val_loss: 0.0229\n",
    "Epoch 5/5\n",
    "76/75 [==============================] - 57s - loss: 0.0251 - val_loss: 0.0225\n",
    "better but still as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) image dimensions, cropped by  65 px from top and  15 px from bot\n",
    "47586 training images out of  55984 overall images [ 15.0 % for validation set]\n",
    "250 batch-size,  0.025 learning rate,  1e-05 decay,  0.85 nesterov-momentum\n",
    "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 20 %, Layer 1-2: 45 %\n",
    "Epoch 1/8\n",
    "96/95 [==============================] - 515s - loss: 0.0536 - val_loss: 0.0411\n",
    "Epoch 2/8\n",
    "96/95 [==============================] - 67s - loss: 0.0414 - val_loss: 0.0380\n",
    "Epoch 3/8\n",
    "96/95 [==============================] - 65s - loss: 0.0387 - val_loss: 0.0364\n",
    "Epoch 4/8\n",
    "96/95 [==============================] - 67s - loss: 0.0370 - val_loss: 0.0353\n",
    "Epoch 5/8\n",
    "96/95 [==============================] - 64s - loss: 0.0357 - val_loss: 0.0349\n",
    "Epoch 6/8\n",
    "96/95 [==============================] - 68s - loss: 0.0347 - val_loss: 0.0337\n",
    "Epoch 7/8\n",
    "96/95 [==============================] - 72s - loss: 0.0335 - val_loss: 0.0328\n",
    "Epoch 8/8\n",
    "96/95 [==============================] - 70s - loss: 0.0327 - val_loss: 0.0324\n",
    "\n",
    "works reliable. a bit too outside of the corner but still without touching lane lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images normalized to [-1,+1]\n",
    "(160, 320, 3) image dimensions, cropped by  65 px from top and  15 px from bot\n",
    "47586 training images out of  55984 overall images [ 15.0 % for validation set]\n",
    "250 batch-size,  0.025 learning rate,  1e-05 decay,  0.85 nesterov-momentum\n",
    "Dropout-Rates: Layer 1-2: 2 %, Layer 1-2: 3 %, Layer 1-2: 20 %, Layer 1-2: 45 %\n",
    "Epoch 1/15\n",
    "96/95 [==============================] - 88s - loss: 0.0589 - val_loss: 0.0402\n",
    "Epoch 2/15\n",
    "96/95 [==============================] - 76s - loss: 0.0432 - val_loss: 0.0374\n",
    "[...]\n",
    "Epoch 13/15\n",
    "96/95 [==============================] - 76s - loss: 0.0298 - val_loss: 0.0274\n",
    "Epoch 14/15\n",
    "96/95 [==============================] - 76s - loss: 0.0292 - val_loss: 0.0271\n",
    "Epoch 15/15\n",
    "96/95 [==============================] - 76s - loss: 0.0289 - val_loss: 0.0266\n",
    "\n",
    "works better, before returning to this, learning rate and dropout rate was increased, resulting in worse losses after 15 epochs, so returned back to the parameters and ran 15 epochs. still no overfit, and parametrers could be evaluated for better parametrization, but task is fulfilled."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
